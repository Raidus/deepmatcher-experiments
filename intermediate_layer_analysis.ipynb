{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepmatcher as dm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -qnc -P sample_data/itunes-amazon https://raw.githubusercontent.com/anhaidgroup/deepmatcher/master/examples/sample_data/itunes-amazon/train.csv\n",
    "!wget -qnc -P sample_data/itunes-amazon https://raw.githubusercontent.com/anhaidgroup/deepmatcher/master/examples/sample_data/itunes-amazon/validation.csv\n",
    "!wget -qnc -P sample_data/itunes-amazon https://raw.githubusercontent.com/anhaidgroup/deepmatcher/master/examples/sample_data/itunes-amazon/test.csv\n",
    "!wget -qnc -P sample_data/itunes-amazon https://raw.githubusercontent.com/anhaidgroup/deepmatcher/master/examples/sample_data/itunes-amazon/unlabeled.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading and processing data from \"sample_data/itunes-amazon/train.csv\"\n",
      "0% [############################# ] 100% | ETA: 00:00:00\n",
      "Reading and processing data from \"sample_data/itunes-amazon/validation.csv\"\n",
      "0% [############################# ] 100% | ETA: 00:00:00\n",
      "Reading and processing data from \"sample_data/itunes-amazon/test.csv\"\n",
      "0% [############################# ] 100% | ETA: 00:00:00\n",
      "Building vocabulary\n",
      "0% [#] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n",
      "\n",
      "Computing principal components\n",
      "0% [#] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'sample_data/itunes-amazon'\n",
    "datasets = dm.data.process(data_dir, train='train.csv', validation='validation.csv',\n",
    "                           test='test.csv')\n",
    "train = datasets[0]\n",
    "validation = datasets[1]\n",
    "test = datasets[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>left_Song_Name</th>\n",
       "      <th>left_Artist_Name</th>\n",
       "      <th>left_Album_Name</th>\n",
       "      <th>left_Genre</th>\n",
       "      <th>left_Price</th>\n",
       "      <th>left_CopyRight</th>\n",
       "      <th>left_Time</th>\n",
       "      <th>left_Released</th>\n",
       "      <th>right_Song_Name</th>\n",
       "      <th>right_Artist_Name</th>\n",
       "      <th>right_Album_Name</th>\n",
       "      <th>right_Genre</th>\n",
       "      <th>right_Price</th>\n",
       "      <th>right_CopyRight</th>\n",
       "      <th>right_Time</th>\n",
       "      <th>right_Released</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>448</td>\n",
       "      <td>0</td>\n",
       "      <td>baby when the light ( david guetta &amp; fred rist...</td>\n",
       "      <td>david guetta</td>\n",
       "      <td>pop life ( extended version ) [ bonus version ]</td>\n",
       "      <td>dance , music , rock , pop , house , electroni...</td>\n",
       "      <td>$ 1.29</td>\n",
       "      <td>‰ ãñ 2007 gum records</td>\n",
       "      <td>6:17</td>\n",
       "      <td>18-sep-07</td>\n",
       "      <td>revolver ( madonna vs. david guetta feat . lil...</td>\n",
       "      <td>david guetta</td>\n",
       "      <td>one love ( deluxe version )</td>\n",
       "      <td>dance &amp; electronic</td>\n",
       "      <td>$ 1.29</td>\n",
       "      <td>( c ) 2014 swedish house mafia holdings ltd ( ...</td>\n",
       "      <td>3:18</td>\n",
       "      <td>august 21 , 2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>287</td>\n",
       "      <td>1</td>\n",
       "      <td>outversion</td>\n",
       "      <td>mark ronson</td>\n",
       "      <td>version</td>\n",
       "      <td>pop , music , r &amp; b / soul , soul , dance , ro...</td>\n",
       "      <td>$ 0.99</td>\n",
       "      <td>2007 mark ronson under exclusive license to so...</td>\n",
       "      <td>1:50</td>\n",
       "      <td>10-jul-07</td>\n",
       "      <td>outversion</td>\n",
       "      <td>mark ronson</td>\n",
       "      <td>version [ explicit ]</td>\n",
       "      <td>pop</td>\n",
       "      <td>$ 0.99</td>\n",
       "      <td>( c ) 2011 j'adore records</td>\n",
       "      <td>1:50</td>\n",
       "      <td>july 10 , 2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>534</td>\n",
       "      <td>0</td>\n",
       "      <td>peer pressure ( feat . traci nelson )</td>\n",
       "      <td>snoop dogg</td>\n",
       "      <td>doggumentary</td>\n",
       "      <td>hip-hop/rap , music , rock , gangsta rap , wes...</td>\n",
       "      <td>$ 1.29</td>\n",
       "      <td>‰ ãñ 2011 capitol records , llc . all rights r...</td>\n",
       "      <td>4:07</td>\n",
       "      <td>29-mar-11</td>\n",
       "      <td>boom ( ( feat . t-pain ) [ edited ] )</td>\n",
       "      <td>snoop dogg</td>\n",
       "      <td>doggumentary [ edited ]</td>\n",
       "      <td>rap &amp; hip-hop , west coast</td>\n",
       "      <td>$ 1.29</td>\n",
       "      <td>( c ) 2011 capitol records , llc</td>\n",
       "      <td>3:50</td>\n",
       "      <td>march 29 , 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>181</td>\n",
       "      <td>1</td>\n",
       "      <td>stars come out ( tim mason remix )</td>\n",
       "      <td>zedd</td>\n",
       "      <td>stars come out ( remixes ) - ep</td>\n",
       "      <td>dance , music , electronic , house</td>\n",
       "      <td>$ 1.29</td>\n",
       "      <td>2012 dim mak inc .</td>\n",
       "      <td>5:49</td>\n",
       "      <td>20-may-14</td>\n",
       "      <td>stars come out ( dillon francis remix )</td>\n",
       "      <td>zedd</td>\n",
       "      <td>stars come out [ dillon francis remix ]</td>\n",
       "      <td>dance &amp; electronic</td>\n",
       "      <td>$ 1.29</td>\n",
       "      <td>2012 dim mak inc .</td>\n",
       "      <td>4:08</td>\n",
       "      <td>may 20 , 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>485</td>\n",
       "      <td>0</td>\n",
       "      <td>jump ( feat . nelly furtado )</td>\n",
       "      <td>flo rida</td>\n",
       "      <td>r.o.o.t.s . ( deluxe version )</td>\n",
       "      <td>hip-hop/rap , music</td>\n",
       "      <td>$ 1.29</td>\n",
       "      <td>‰ ãñ 2009 atlantic recording corporation for t...</td>\n",
       "      <td>3:28</td>\n",
       "      <td>30-mar-09</td>\n",
       "      <td>yayo [ feat . brisco , billy blue , ball greez...</td>\n",
       "      <td>flo rida</td>\n",
       "      <td>r.o.o.t.s . ( route of overcoming the struggle...</td>\n",
       "      <td>rap &amp; hip-hop</td>\n",
       "      <td>$ 1.29</td>\n",
       "      <td>( c ) 2012 motown records , a division of umg ...</td>\n",
       "      <td>7:53</td>\n",
       "      <td>march 30 , 2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  label                                     left_Song_Name  \\\n",
       "0  448      0  baby when the light ( david guetta & fred rist...   \n",
       "1  287      1                                         outversion   \n",
       "2  534      0              peer pressure ( feat . traci nelson )   \n",
       "3  181      1                 stars come out ( tim mason remix )   \n",
       "4  485      0                      jump ( feat . nelly furtado )   \n",
       "\n",
       "  left_Artist_Name                                  left_Album_Name  \\\n",
       "0     david guetta  pop life ( extended version ) [ bonus version ]   \n",
       "1      mark ronson                                          version   \n",
       "2       snoop dogg                                     doggumentary   \n",
       "3             zedd                  stars come out ( remixes ) - ep   \n",
       "4         flo rida                   r.o.o.t.s . ( deluxe version )   \n",
       "\n",
       "                                          left_Genre left_Price  \\\n",
       "0  dance , music , rock , pop , house , electroni...     $ 1.29   \n",
       "1  pop , music , r & b / soul , soul , dance , ro...     $ 0.99   \n",
       "2  hip-hop/rap , music , rock , gangsta rap , wes...     $ 1.29   \n",
       "3                 dance , music , electronic , house     $ 1.29   \n",
       "4                                hip-hop/rap , music     $ 1.29   \n",
       "\n",
       "                                      left_CopyRight left_Time left_Released  \\\n",
       "0                              ‰ ãñ 2007 gum records      6:17     18-sep-07   \n",
       "1  2007 mark ronson under exclusive license to so...      1:50     10-jul-07   \n",
       "2  ‰ ãñ 2011 capitol records , llc . all rights r...      4:07     29-mar-11   \n",
       "3                                 2012 dim mak inc .      5:49     20-may-14   \n",
       "4  ‰ ãñ 2009 atlantic recording corporation for t...      3:28     30-mar-09   \n",
       "\n",
       "                                     right_Song_Name right_Artist_Name  \\\n",
       "0  revolver ( madonna vs. david guetta feat . lil...      david guetta   \n",
       "1                                         outversion       mark ronson   \n",
       "2              boom ( ( feat . t-pain ) [ edited ] )        snoop dogg   \n",
       "3            stars come out ( dillon francis remix )              zedd   \n",
       "4  yayo [ feat . brisco , billy blue , ball greez...          flo rida   \n",
       "\n",
       "                                    right_Album_Name  \\\n",
       "0                        one love ( deluxe version )   \n",
       "1                               version [ explicit ]   \n",
       "2                            doggumentary [ edited ]   \n",
       "3            stars come out [ dillon francis remix ]   \n",
       "4  r.o.o.t.s . ( route of overcoming the struggle...   \n",
       "\n",
       "                  right_Genre right_Price  \\\n",
       "0          dance & electronic      $ 1.29   \n",
       "1                         pop      $ 0.99   \n",
       "2  rap & hip-hop , west coast      $ 1.29   \n",
       "3          dance & electronic      $ 1.29   \n",
       "4               rap & hip-hop      $ 1.29   \n",
       "\n",
       "                                     right_CopyRight right_Time  \\\n",
       "0  ( c ) 2014 swedish house mafia holdings ltd ( ...       3:18   \n",
       "1                         ( c ) 2011 j'adore records       1:50   \n",
       "2                   ( c ) 2011 capitol records , llc       3:50   \n",
       "3                                 2012 dim mak inc .       4:08   \n",
       "4  ( c ) 2012 motown records , a division of umg ...       7:53   \n",
       "\n",
       "     right_Released  \n",
       "0  august 21 , 2009  \n",
       "1    july 10 , 2007  \n",
       "2   march 29 , 2011  \n",
       "3     may 20 , 2014  \n",
       "4   march 30 , 2009  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_table = train.get_raw_table()\n",
    "train_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define neural network models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_model = dm.MatchingModel(attr_summarizer='hybrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MatchingModel(\n",
       "  (attr_summarizers): ModuleMap(\n",
       "    (Song_Name): Hybrid(\n",
       "      (word_contextualizer): RNN(\n",
       "        (rnn_groups): ModuleList(\n",
       "          (0): GRU(300, 150, batch_first=True, bidirectional=True)\n",
       "        )\n",
       "        (dropouts): ModuleList(\n",
       "          (0): Dropout(p=0)\n",
       "        )\n",
       "        (bypass_networks): ModuleList(\n",
       "          (0): None\n",
       "        )\n",
       "        (input_dropout): NoMeta(\n",
       "          (module): Dropout(p=0)\n",
       "        )\n",
       "      )\n",
       "      (word_comparator): Attention(\n",
       "        (alignment_networks): ModuleList(\n",
       "          (0): AlignmentNetwork(\n",
       "            (transform): Transform(\n",
       "              (transforms): ModuleList(\n",
       "                (0): Linear(in_features=300, out_features=300, bias=True)\n",
       "                (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "              )\n",
       "              (bypass_networks): ModuleList(\n",
       "                (0): Bypass(\n",
       "                  (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "                )\n",
       "                (1): Bypass(\n",
       "                  (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (value_merge): Merge(\n",
       "        )\n",
       "        (comparison_merge): Merge(\n",
       "        )\n",
       "        (comparison_network): Transform(\n",
       "          (transforms): ModuleList(\n",
       "            (0): Linear(in_features=600, out_features=300, bias=True)\n",
       "            (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): Bypass(\n",
       "              (highway_gate): Linear(in_features=600, out_features=300, bias=True)\n",
       "            )\n",
       "            (1): Bypass(\n",
       "              (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_dropout): Dropout(p=0)\n",
       "        (transform_dropout): Dropout(p=0)\n",
       "        (score_dropout): Dropout(p=0)\n",
       "        (softmax): Softmax()\n",
       "      )\n",
       "      (word_aggregator): AttentionWithRNN(\n",
       "        (rnn): RNN(\n",
       "          (rnn_groups): ModuleList(\n",
       "            (0): GRU(300, 150, batch_first=True, bidirectional=True)\n",
       "          )\n",
       "          (dropouts): ModuleList(\n",
       "            (0): Dropout(p=0)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): None\n",
       "          )\n",
       "          (input_dropout): NoMeta(\n",
       "            (module): Dropout(p=0)\n",
       "          )\n",
       "        )\n",
       "        (rnn_pool): Pool(\n",
       "        )\n",
       "        (input_context_comparison_network): Transform(\n",
       "          (transforms): ModuleList(\n",
       "            (0): Linear(in_features=600, out_features=300, bias=True)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): Bypass(\n",
       "              (highway_gate): Linear(in_features=600, out_features=300, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (scoring_network): Transform(\n",
       "          (transforms): ModuleList(\n",
       "            (0): Linear(in_features=300, out_features=1, bias=True)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): None\n",
       "          )\n",
       "        )\n",
       "        (input_dropout): Dropout(p=0)\n",
       "        (transform_dropout): Dropout(p=0)\n",
       "        (score_dropout): Dropout(p=0)\n",
       "        (softmax): Softmax()\n",
       "      )\n",
       "    )\n",
       "    (Artist_Name): Hybrid(\n",
       "      (word_contextualizer): RNN(\n",
       "        (rnn_groups): ModuleList(\n",
       "          (0): GRU(300, 150, batch_first=True, bidirectional=True)\n",
       "        )\n",
       "        (dropouts): ModuleList(\n",
       "          (0): Dropout(p=0)\n",
       "        )\n",
       "        (bypass_networks): ModuleList(\n",
       "          (0): None\n",
       "        )\n",
       "        (input_dropout): NoMeta(\n",
       "          (module): Dropout(p=0)\n",
       "        )\n",
       "      )\n",
       "      (word_comparator): Attention(\n",
       "        (alignment_networks): ModuleList(\n",
       "          (0): AlignmentNetwork(\n",
       "            (transform): Transform(\n",
       "              (transforms): ModuleList(\n",
       "                (0): Linear(in_features=300, out_features=300, bias=True)\n",
       "                (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "              )\n",
       "              (bypass_networks): ModuleList(\n",
       "                (0): Bypass(\n",
       "                  (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "                )\n",
       "                (1): Bypass(\n",
       "                  (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (value_merge): Merge(\n",
       "        )\n",
       "        (comparison_merge): Merge(\n",
       "        )\n",
       "        (comparison_network): Transform(\n",
       "          (transforms): ModuleList(\n",
       "            (0): Linear(in_features=600, out_features=300, bias=True)\n",
       "            (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): Bypass(\n",
       "              (highway_gate): Linear(in_features=600, out_features=300, bias=True)\n",
       "            )\n",
       "            (1): Bypass(\n",
       "              (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_dropout): Dropout(p=0)\n",
       "        (transform_dropout): Dropout(p=0)\n",
       "        (score_dropout): Dropout(p=0)\n",
       "        (softmax): Softmax()\n",
       "      )\n",
       "      (word_aggregator): AttentionWithRNN(\n",
       "        (rnn): RNN(\n",
       "          (rnn_groups): ModuleList(\n",
       "            (0): GRU(300, 150, batch_first=True, bidirectional=True)\n",
       "          )\n",
       "          (dropouts): ModuleList(\n",
       "            (0): Dropout(p=0)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): None\n",
       "          )\n",
       "          (input_dropout): NoMeta(\n",
       "            (module): Dropout(p=0)\n",
       "          )\n",
       "        )\n",
       "        (rnn_pool): Pool(\n",
       "        )\n",
       "        (input_context_comparison_network): Transform(\n",
       "          (transforms): ModuleList(\n",
       "            (0): Linear(in_features=600, out_features=300, bias=True)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): Bypass(\n",
       "              (highway_gate): Linear(in_features=600, out_features=300, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (scoring_network): Transform(\n",
       "          (transforms): ModuleList(\n",
       "            (0): Linear(in_features=300, out_features=1, bias=True)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): None\n",
       "          )\n",
       "        )\n",
       "        (input_dropout): Dropout(p=0)\n",
       "        (transform_dropout): Dropout(p=0)\n",
       "        (score_dropout): Dropout(p=0)\n",
       "        (softmax): Softmax()\n",
       "      )\n",
       "    )\n",
       "    (Album_Name): Hybrid(\n",
       "      (word_contextualizer): RNN(\n",
       "        (rnn_groups): ModuleList(\n",
       "          (0): GRU(300, 150, batch_first=True, bidirectional=True)\n",
       "        )\n",
       "        (dropouts): ModuleList(\n",
       "          (0): Dropout(p=0)\n",
       "        )\n",
       "        (bypass_networks): ModuleList(\n",
       "          (0): None\n",
       "        )\n",
       "        (input_dropout): NoMeta(\n",
       "          (module): Dropout(p=0)\n",
       "        )\n",
       "      )\n",
       "      (word_comparator): Attention(\n",
       "        (alignment_networks): ModuleList(\n",
       "          (0): AlignmentNetwork(\n",
       "            (transform): Transform(\n",
       "              (transforms): ModuleList(\n",
       "                (0): Linear(in_features=300, out_features=300, bias=True)\n",
       "                (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "              )\n",
       "              (bypass_networks): ModuleList(\n",
       "                (0): Bypass(\n",
       "                  (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "                )\n",
       "                (1): Bypass(\n",
       "                  (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (value_merge): Merge(\n",
       "        )\n",
       "        (comparison_merge): Merge(\n",
       "        )\n",
       "        (comparison_network): Transform(\n",
       "          (transforms): ModuleList(\n",
       "            (0): Linear(in_features=600, out_features=300, bias=True)\n",
       "            (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): Bypass(\n",
       "              (highway_gate): Linear(in_features=600, out_features=300, bias=True)\n",
       "            )\n",
       "            (1): Bypass(\n",
       "              (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_dropout): Dropout(p=0)\n",
       "        (transform_dropout): Dropout(p=0)\n",
       "        (score_dropout): Dropout(p=0)\n",
       "        (softmax): Softmax()\n",
       "      )\n",
       "      (word_aggregator): AttentionWithRNN(\n",
       "        (rnn): RNN(\n",
       "          (rnn_groups): ModuleList(\n",
       "            (0): GRU(300, 150, batch_first=True, bidirectional=True)\n",
       "          )\n",
       "          (dropouts): ModuleList(\n",
       "            (0): Dropout(p=0)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): None\n",
       "          )\n",
       "          (input_dropout): NoMeta(\n",
       "            (module): Dropout(p=0)\n",
       "          )\n",
       "        )\n",
       "        (rnn_pool): Pool(\n",
       "        )\n",
       "        (input_context_comparison_network): Transform(\n",
       "          (transforms): ModuleList(\n",
       "            (0): Linear(in_features=600, out_features=300, bias=True)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): Bypass(\n",
       "              (highway_gate): Linear(in_features=600, out_features=300, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (scoring_network): Transform(\n",
       "          (transforms): ModuleList(\n",
       "            (0): Linear(in_features=300, out_features=1, bias=True)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): None\n",
       "          )\n",
       "        )\n",
       "        (input_dropout): Dropout(p=0)\n",
       "        (transform_dropout): Dropout(p=0)\n",
       "        (score_dropout): Dropout(p=0)\n",
       "        (softmax): Softmax()\n",
       "      )\n",
       "    )\n",
       "    (Genre): Hybrid(\n",
       "      (word_contextualizer): RNN(\n",
       "        (rnn_groups): ModuleList(\n",
       "          (0): GRU(300, 150, batch_first=True, bidirectional=True)\n",
       "        )\n",
       "        (dropouts): ModuleList(\n",
       "          (0): Dropout(p=0)\n",
       "        )\n",
       "        (bypass_networks): ModuleList(\n",
       "          (0): None\n",
       "        )\n",
       "        (input_dropout): NoMeta(\n",
       "          (module): Dropout(p=0)\n",
       "        )\n",
       "      )\n",
       "      (word_comparator): Attention(\n",
       "        (alignment_networks): ModuleList(\n",
       "          (0): AlignmentNetwork(\n",
       "            (transform): Transform(\n",
       "              (transforms): ModuleList(\n",
       "                (0): Linear(in_features=300, out_features=300, bias=True)\n",
       "                (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "              )\n",
       "              (bypass_networks): ModuleList(\n",
       "                (0): Bypass(\n",
       "                  (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "                )\n",
       "                (1): Bypass(\n",
       "                  (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (value_merge): Merge(\n",
       "        )\n",
       "        (comparison_merge): Merge(\n",
       "        )\n",
       "        (comparison_network): Transform(\n",
       "          (transforms): ModuleList(\n",
       "            (0): Linear(in_features=600, out_features=300, bias=True)\n",
       "            (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): Bypass(\n",
       "              (highway_gate): Linear(in_features=600, out_features=300, bias=True)\n",
       "            )\n",
       "            (1): Bypass(\n",
       "              (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_dropout): Dropout(p=0)\n",
       "        (transform_dropout): Dropout(p=0)\n",
       "        (score_dropout): Dropout(p=0)\n",
       "        (softmax): Softmax()\n",
       "      )\n",
       "      (word_aggregator): AttentionWithRNN(\n",
       "        (rnn): RNN(\n",
       "          (rnn_groups): ModuleList(\n",
       "            (0): GRU(300, 150, batch_first=True, bidirectional=True)\n",
       "          )\n",
       "          (dropouts): ModuleList(\n",
       "            (0): Dropout(p=0)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): None\n",
       "          )\n",
       "          (input_dropout): NoMeta(\n",
       "            (module): Dropout(p=0)\n",
       "          )\n",
       "        )\n",
       "        (rnn_pool): Pool(\n",
       "        )\n",
       "        (input_context_comparison_network): Transform(\n",
       "          (transforms): ModuleList(\n",
       "            (0): Linear(in_features=600, out_features=300, bias=True)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): Bypass(\n",
       "              (highway_gate): Linear(in_features=600, out_features=300, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (scoring_network): Transform(\n",
       "          (transforms): ModuleList(\n",
       "            (0): Linear(in_features=300, out_features=1, bias=True)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): None\n",
       "          )\n",
       "        )\n",
       "        (input_dropout): Dropout(p=0)\n",
       "        (transform_dropout): Dropout(p=0)\n",
       "        (score_dropout): Dropout(p=0)\n",
       "        (softmax): Softmax()\n",
       "      )\n",
       "    )\n",
       "    (Price): Hybrid(\n",
       "      (word_contextualizer): RNN(\n",
       "        (rnn_groups): ModuleList(\n",
       "          (0): GRU(300, 150, batch_first=True, bidirectional=True)\n",
       "        )\n",
       "        (dropouts): ModuleList(\n",
       "          (0): Dropout(p=0)\n",
       "        )\n",
       "        (bypass_networks): ModuleList(\n",
       "          (0): None\n",
       "        )\n",
       "        (input_dropout): NoMeta(\n",
       "          (module): Dropout(p=0)\n",
       "        )\n",
       "      )\n",
       "      (word_comparator): Attention(\n",
       "        (alignment_networks): ModuleList(\n",
       "          (0): AlignmentNetwork(\n",
       "            (transform): Transform(\n",
       "              (transforms): ModuleList(\n",
       "                (0): Linear(in_features=300, out_features=300, bias=True)\n",
       "                (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "              )\n",
       "              (bypass_networks): ModuleList(\n",
       "                (0): Bypass(\n",
       "                  (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "                )\n",
       "                (1): Bypass(\n",
       "                  (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (value_merge): Merge(\n",
       "        )\n",
       "        (comparison_merge): Merge(\n",
       "        )\n",
       "        (comparison_network): Transform(\n",
       "          (transforms): ModuleList(\n",
       "            (0): Linear(in_features=600, out_features=300, bias=True)\n",
       "            (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): Bypass(\n",
       "              (highway_gate): Linear(in_features=600, out_features=300, bias=True)\n",
       "            )\n",
       "            (1): Bypass(\n",
       "              (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_dropout): Dropout(p=0)\n",
       "        (transform_dropout): Dropout(p=0)\n",
       "        (score_dropout): Dropout(p=0)\n",
       "        (softmax): Softmax()\n",
       "      )\n",
       "      (word_aggregator): AttentionWithRNN(\n",
       "        (rnn): RNN(\n",
       "          (rnn_groups): ModuleList(\n",
       "            (0): GRU(300, 150, batch_first=True, bidirectional=True)\n",
       "          )\n",
       "          (dropouts): ModuleList(\n",
       "            (0): Dropout(p=0)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): None\n",
       "          )\n",
       "          (input_dropout): NoMeta(\n",
       "            (module): Dropout(p=0)\n",
       "          )\n",
       "        )\n",
       "        (rnn_pool): Pool(\n",
       "        )\n",
       "        (input_context_comparison_network): Transform(\n",
       "          (transforms): ModuleList(\n",
       "            (0): Linear(in_features=600, out_features=300, bias=True)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): Bypass(\n",
       "              (highway_gate): Linear(in_features=600, out_features=300, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (scoring_network): Transform(\n",
       "          (transforms): ModuleList(\n",
       "            (0): Linear(in_features=300, out_features=1, bias=True)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): None\n",
       "          )\n",
       "        )\n",
       "        (input_dropout): Dropout(p=0)\n",
       "        (transform_dropout): Dropout(p=0)\n",
       "        (score_dropout): Dropout(p=0)\n",
       "        (softmax): Softmax()\n",
       "      )\n",
       "    )\n",
       "    (CopyRight): Hybrid(\n",
       "      (word_contextualizer): RNN(\n",
       "        (rnn_groups): ModuleList(\n",
       "          (0): GRU(300, 150, batch_first=True, bidirectional=True)\n",
       "        )\n",
       "        (dropouts): ModuleList(\n",
       "          (0): Dropout(p=0)\n",
       "        )\n",
       "        (bypass_networks): ModuleList(\n",
       "          (0): None\n",
       "        )\n",
       "        (input_dropout): NoMeta(\n",
       "          (module): Dropout(p=0)\n",
       "        )\n",
       "      )\n",
       "      (word_comparator): Attention(\n",
       "        (alignment_networks): ModuleList(\n",
       "          (0): AlignmentNetwork(\n",
       "            (transform): Transform(\n",
       "              (transforms): ModuleList(\n",
       "                (0): Linear(in_features=300, out_features=300, bias=True)\n",
       "                (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "              )\n",
       "              (bypass_networks): ModuleList(\n",
       "                (0): Bypass(\n",
       "                  (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "                )\n",
       "                (1): Bypass(\n",
       "                  (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (value_merge): Merge(\n",
       "        )\n",
       "        (comparison_merge): Merge(\n",
       "        )\n",
       "        (comparison_network): Transform(\n",
       "          (transforms): ModuleList(\n",
       "            (0): Linear(in_features=600, out_features=300, bias=True)\n",
       "            (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): Bypass(\n",
       "              (highway_gate): Linear(in_features=600, out_features=300, bias=True)\n",
       "            )\n",
       "            (1): Bypass(\n",
       "              (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_dropout): Dropout(p=0)\n",
       "        (transform_dropout): Dropout(p=0)\n",
       "        (score_dropout): Dropout(p=0)\n",
       "        (softmax): Softmax()\n",
       "      )\n",
       "      (word_aggregator): AttentionWithRNN(\n",
       "        (rnn): RNN(\n",
       "          (rnn_groups): ModuleList(\n",
       "            (0): GRU(300, 150, batch_first=True, bidirectional=True)\n",
       "          )\n",
       "          (dropouts): ModuleList(\n",
       "            (0): Dropout(p=0)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): None\n",
       "          )\n",
       "          (input_dropout): NoMeta(\n",
       "            (module): Dropout(p=0)\n",
       "          )\n",
       "        )\n",
       "        (rnn_pool): Pool(\n",
       "        )\n",
       "        (input_context_comparison_network): Transform(\n",
       "          (transforms): ModuleList(\n",
       "            (0): Linear(in_features=600, out_features=300, bias=True)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): Bypass(\n",
       "              (highway_gate): Linear(in_features=600, out_features=300, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (scoring_network): Transform(\n",
       "          (transforms): ModuleList(\n",
       "            (0): Linear(in_features=300, out_features=1, bias=True)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): None\n",
       "          )\n",
       "        )\n",
       "        (input_dropout): Dropout(p=0)\n",
       "        (transform_dropout): Dropout(p=0)\n",
       "        (score_dropout): Dropout(p=0)\n",
       "        (softmax): Softmax()\n",
       "      )\n",
       "    )\n",
       "    (Time): Hybrid(\n",
       "      (word_contextualizer): RNN(\n",
       "        (rnn_groups): ModuleList(\n",
       "          (0): GRU(300, 150, batch_first=True, bidirectional=True)\n",
       "        )\n",
       "        (dropouts): ModuleList(\n",
       "          (0): Dropout(p=0)\n",
       "        )\n",
       "        (bypass_networks): ModuleList(\n",
       "          (0): None\n",
       "        )\n",
       "        (input_dropout): NoMeta(\n",
       "          (module): Dropout(p=0)\n",
       "        )\n",
       "      )\n",
       "      (word_comparator): Attention(\n",
       "        (alignment_networks): ModuleList(\n",
       "          (0): AlignmentNetwork(\n",
       "            (transform): Transform(\n",
       "              (transforms): ModuleList(\n",
       "                (0): Linear(in_features=300, out_features=300, bias=True)\n",
       "                (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "              )\n",
       "              (bypass_networks): ModuleList(\n",
       "                (0): Bypass(\n",
       "                  (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "                )\n",
       "                (1): Bypass(\n",
       "                  (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (value_merge): Merge(\n",
       "        )\n",
       "        (comparison_merge): Merge(\n",
       "        )\n",
       "        (comparison_network): Transform(\n",
       "          (transforms): ModuleList(\n",
       "            (0): Linear(in_features=600, out_features=300, bias=True)\n",
       "            (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): Bypass(\n",
       "              (highway_gate): Linear(in_features=600, out_features=300, bias=True)\n",
       "            )\n",
       "            (1): Bypass(\n",
       "              (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_dropout): Dropout(p=0)\n",
       "        (transform_dropout): Dropout(p=0)\n",
       "        (score_dropout): Dropout(p=0)\n",
       "        (softmax): Softmax()\n",
       "      )\n",
       "      (word_aggregator): AttentionWithRNN(\n",
       "        (rnn): RNN(\n",
       "          (rnn_groups): ModuleList(\n",
       "            (0): GRU(300, 150, batch_first=True, bidirectional=True)\n",
       "          )\n",
       "          (dropouts): ModuleList(\n",
       "            (0): Dropout(p=0)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): None\n",
       "          )\n",
       "          (input_dropout): NoMeta(\n",
       "            (module): Dropout(p=0)\n",
       "          )\n",
       "        )\n",
       "        (rnn_pool): Pool(\n",
       "        )\n",
       "        (input_context_comparison_network): Transform(\n",
       "          (transforms): ModuleList(\n",
       "            (0): Linear(in_features=600, out_features=300, bias=True)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): Bypass(\n",
       "              (highway_gate): Linear(in_features=600, out_features=300, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (scoring_network): Transform(\n",
       "          (transforms): ModuleList(\n",
       "            (0): Linear(in_features=300, out_features=1, bias=True)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): None\n",
       "          )\n",
       "        )\n",
       "        (input_dropout): Dropout(p=0)\n",
       "        (transform_dropout): Dropout(p=0)\n",
       "        (score_dropout): Dropout(p=0)\n",
       "        (softmax): Softmax()\n",
       "      )\n",
       "    )\n",
       "    (Released): Hybrid(\n",
       "      (word_contextualizer): RNN(\n",
       "        (rnn_groups): ModuleList(\n",
       "          (0): GRU(300, 150, batch_first=True, bidirectional=True)\n",
       "        )\n",
       "        (dropouts): ModuleList(\n",
       "          (0): Dropout(p=0)\n",
       "        )\n",
       "        (bypass_networks): ModuleList(\n",
       "          (0): None\n",
       "        )\n",
       "        (input_dropout): NoMeta(\n",
       "          (module): Dropout(p=0)\n",
       "        )\n",
       "      )\n",
       "      (word_comparator): Attention(\n",
       "        (alignment_networks): ModuleList(\n",
       "          (0): AlignmentNetwork(\n",
       "            (transform): Transform(\n",
       "              (transforms): ModuleList(\n",
       "                (0): Linear(in_features=300, out_features=300, bias=True)\n",
       "                (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "              )\n",
       "              (bypass_networks): ModuleList(\n",
       "                (0): Bypass(\n",
       "                  (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "                )\n",
       "                (1): Bypass(\n",
       "                  (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (value_merge): Merge(\n",
       "        )\n",
       "        (comparison_merge): Merge(\n",
       "        )\n",
       "        (comparison_network): Transform(\n",
       "          (transforms): ModuleList(\n",
       "            (0): Linear(in_features=600, out_features=300, bias=True)\n",
       "            (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): Bypass(\n",
       "              (highway_gate): Linear(in_features=600, out_features=300, bias=True)\n",
       "            )\n",
       "            (1): Bypass(\n",
       "              (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_dropout): Dropout(p=0)\n",
       "        (transform_dropout): Dropout(p=0)\n",
       "        (score_dropout): Dropout(p=0)\n",
       "        (softmax): Softmax()\n",
       "      )\n",
       "      (word_aggregator): AttentionWithRNN(\n",
       "        (rnn): RNN(\n",
       "          (rnn_groups): ModuleList(\n",
       "            (0): GRU(300, 150, batch_first=True, bidirectional=True)\n",
       "          )\n",
       "          (dropouts): ModuleList(\n",
       "            (0): Dropout(p=0)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): None\n",
       "          )\n",
       "          (input_dropout): NoMeta(\n",
       "            (module): Dropout(p=0)\n",
       "          )\n",
       "        )\n",
       "        (rnn_pool): Pool(\n",
       "        )\n",
       "        (input_context_comparison_network): Transform(\n",
       "          (transforms): ModuleList(\n",
       "            (0): Linear(in_features=600, out_features=300, bias=True)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): Bypass(\n",
       "              (highway_gate): Linear(in_features=600, out_features=300, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (scoring_network): Transform(\n",
       "          (transforms): ModuleList(\n",
       "            (0): Linear(in_features=300, out_features=1, bias=True)\n",
       "          )\n",
       "          (bypass_networks): ModuleList(\n",
       "            (0): None\n",
       "          )\n",
       "        )\n",
       "        (input_dropout): Dropout(p=0)\n",
       "        (transform_dropout): Dropout(p=0)\n",
       "        (score_dropout): Dropout(p=0)\n",
       "        (softmax): Softmax()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (attr_summarizer): Hybrid(\n",
       "  )\n",
       "  (attr_condensors): ModuleMap(\n",
       "    (Song_Name): Transform(\n",
       "      (transforms): ModuleList(\n",
       "        (0): Linear(in_features=300, out_features=50, bias=True)\n",
       "      )\n",
       "      (bypass_networks): ModuleList(\n",
       "        (0): Bypass(\n",
       "          (highway_gate): Linear(in_features=300, out_features=50, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (Artist_Name): Transform(\n",
       "      (transforms): ModuleList(\n",
       "        (0): Linear(in_features=300, out_features=50, bias=True)\n",
       "      )\n",
       "      (bypass_networks): ModuleList(\n",
       "        (0): Bypass(\n",
       "          (highway_gate): Linear(in_features=300, out_features=50, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (Album_Name): Transform(\n",
       "      (transforms): ModuleList(\n",
       "        (0): Linear(in_features=300, out_features=50, bias=True)\n",
       "      )\n",
       "      (bypass_networks): ModuleList(\n",
       "        (0): Bypass(\n",
       "          (highway_gate): Linear(in_features=300, out_features=50, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (Genre): Transform(\n",
       "      (transforms): ModuleList(\n",
       "        (0): Linear(in_features=300, out_features=50, bias=True)\n",
       "      )\n",
       "      (bypass_networks): ModuleList(\n",
       "        (0): Bypass(\n",
       "          (highway_gate): Linear(in_features=300, out_features=50, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (Price): Transform(\n",
       "      (transforms): ModuleList(\n",
       "        (0): Linear(in_features=300, out_features=50, bias=True)\n",
       "      )\n",
       "      (bypass_networks): ModuleList(\n",
       "        (0): Bypass(\n",
       "          (highway_gate): Linear(in_features=300, out_features=50, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (CopyRight): Transform(\n",
       "      (transforms): ModuleList(\n",
       "        (0): Linear(in_features=300, out_features=50, bias=True)\n",
       "      )\n",
       "      (bypass_networks): ModuleList(\n",
       "        (0): Bypass(\n",
       "          (highway_gate): Linear(in_features=300, out_features=50, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (Time): Transform(\n",
       "      (transforms): ModuleList(\n",
       "        (0): Linear(in_features=300, out_features=50, bias=True)\n",
       "      )\n",
       "      (bypass_networks): ModuleList(\n",
       "        (0): Bypass(\n",
       "          (highway_gate): Linear(in_features=300, out_features=50, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (Released): Transform(\n",
       "      (transforms): ModuleList(\n",
       "        (0): Linear(in_features=300, out_features=50, bias=True)\n",
       "      )\n",
       "      (bypass_networks): ModuleList(\n",
       "        (0): Bypass(\n",
       "          (highway_gate): Linear(in_features=300, out_features=50, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (attr_comparators): ModuleMap(\n",
       "    (Song_Name): Merge(\n",
       "    )\n",
       "    (Artist_Name): Merge(\n",
       "    )\n",
       "    (Album_Name): Merge(\n",
       "    )\n",
       "    (Genre): Merge(\n",
       "    )\n",
       "    (Price): Merge(\n",
       "    )\n",
       "    (CopyRight): Merge(\n",
       "    )\n",
       "    (Time): Merge(\n",
       "    )\n",
       "    (Released): Merge(\n",
       "    )\n",
       "  )\n",
       "  (attr_comparator): Merge(\n",
       "  )\n",
       "  (attr_merge): Merge(\n",
       "  )\n",
       "  (classifier): Classifier(\n",
       "    (transform): Transform(\n",
       "      (transforms): ModuleList(\n",
       "        (0): Linear(in_features=1200, out_features=300, bias=True)\n",
       "        (1): Linear(in_features=300, out_features=300, bias=True)\n",
       "      )\n",
       "      (bypass_networks): ModuleList(\n",
       "        (0): Bypass(\n",
       "          (highway_gate): Linear(in_features=1200, out_features=300, bias=True)\n",
       "        )\n",
       "        (1): Bypass(\n",
       "          (highway_gate): Linear(in_features=300, out_features=300, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (softmax_transform): Transform(\n",
       "      (transforms): ModuleList(\n",
       "        (0): Linear(in_features=300, out_features=2, bias=True)\n",
       "      )\n",
       "      (bypass_networks): ModuleList(\n",
       "        (0): None\n",
       "      )\n",
       "    )\n",
       "    (softmax): LogSoftmax()\n",
       "  )\n",
       "  (embed): ModuleMap(\n",
       "    (left_Song_Name): NoMeta(\n",
       "      (module): Embedding(2227, 300)\n",
       "    )\n",
       "    (left_Artist_Name): NoMeta(\n",
       "      (module): Embedding(2227, 300)\n",
       "    )\n",
       "    (left_Album_Name): NoMeta(\n",
       "      (module): Embedding(2227, 300)\n",
       "    )\n",
       "    (left_Genre): NoMeta(\n",
       "      (module): Embedding(2227, 300)\n",
       "    )\n",
       "    (left_Price): NoMeta(\n",
       "      (module): Embedding(2227, 300)\n",
       "    )\n",
       "    (left_CopyRight): NoMeta(\n",
       "      (module): Embedding(2227, 300)\n",
       "    )\n",
       "    (left_Time): NoMeta(\n",
       "      (module): Embedding(2227, 300)\n",
       "    )\n",
       "    (left_Released): NoMeta(\n",
       "      (module): Embedding(2227, 300)\n",
       "    )\n",
       "    (right_Song_Name): NoMeta(\n",
       "      (module): Embedding(2227, 300)\n",
       "    )\n",
       "    (right_Artist_Name): NoMeta(\n",
       "      (module): Embedding(2227, 300)\n",
       "    )\n",
       "    (right_Album_Name): NoMeta(\n",
       "      (module): Embedding(2227, 300)\n",
       "    )\n",
       "    (right_Genre): NoMeta(\n",
       "      (module): Embedding(2227, 300)\n",
       "    )\n",
       "    (right_Price): NoMeta(\n",
       "      (module): Embedding(2227, 300)\n",
       "    )\n",
       "    (right_CopyRight): NoMeta(\n",
       "      (module): Embedding(2227, 300)\n",
       "    )\n",
       "    (right_Time): NoMeta(\n",
       "      (module): Embedding(2227, 300)\n",
       "    )\n",
       "    (right_Released): NoMeta(\n",
       "      (module): Embedding(2227, 300)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run this cell if you want to load pre-trained model\n",
    "hybrid_model.load_state('hybrid_model.pth')\n",
    "hybrid_model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Number of trainable parameters: 17757810\n",
      "===>  TRAIN Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 1 || Run Time:    5.9 | Load Time:    0.6 || F1:  39.02 | Prec:  28.40 | Rec:  62.34 || Ex/s:  49.29\n",
      "\n",
      "===>  EVAL Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [█] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 1 || Run Time:    0.6 | Load Time:    0.2 || F1:  67.92 | Prec:  62.07 | Rec:  75.00 || Ex/s: 132.04\n",
      "\n",
      "* Best F1: 67.92452830188678\n",
      "Saving best model...\n",
      "Done.\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 2 || Run Time:    4.6 | Load Time:    0.6 || F1:  71.35 | Prec:  61.11 | Rec:  85.71 || Ex/s:  62.70\n",
      "\n",
      "===>  EVAL Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [█] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 2 || Run Time:    0.6 | Load Time:    0.2 || F1:  76.00 | Prec:  73.08 | Rec:  79.17 || Ex/s: 134.46\n",
      "\n",
      "* Best F1: 76.0\n",
      "Saving best model...\n",
      "Done.\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 3 || Run Time:    4.8 | Load Time:    0.6 || F1:  88.62 | Prec:  82.22 | Rec:  96.10 || Ex/s:  59.96\n",
      "\n",
      "===>  EVAL Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [█] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 3 || Run Time:    0.6 | Load Time:    0.2 || F1:  80.00 | Prec:  76.92 | Rec:  83.33 || Ex/s: 134.90\n",
      "\n",
      "* Best F1: 79.99999999999999\n",
      "Saving best model...\n",
      "Done.\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 4 || Run Time:    4.9 | Load Time:    0.6 || F1:  92.50 | Prec:  89.16 | Rec:  96.10 || Ex/s:  59.12\n",
      "\n",
      "===>  EVAL Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [█] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 4 || Run Time:    0.6 | Load Time:    0.2 || F1:  81.63 | Prec:  80.00 | Rec:  83.33 || Ex/s: 127.19\n",
      "\n",
      "* Best F1: 81.63265306122449\n",
      "Saving best model...\n",
      "Done.\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 5 || Run Time:    4.9 | Load Time:    0.6 || F1:  95.65 | Prec:  91.67 | Rec: 100.00 || Ex/s:  58.97\n",
      "\n",
      "===>  EVAL Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [█] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 5 || Run Time:    0.7 | Load Time:    0.2 || F1:  77.27 | Prec:  85.00 | Rec:  70.83 || Ex/s: 119.79\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 6 || Run Time:    4.7 | Load Time:    0.6 || F1:  97.47 | Prec:  95.06 | Rec: 100.00 || Ex/s:  60.84\n",
      "\n",
      "===>  EVAL Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [█] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 6 || Run Time:    0.6 | Load Time:    0.2 || F1:  84.44 | Prec:  90.48 | Rec:  79.17 || Ex/s: 137.66\n",
      "\n",
      "* Best F1: 84.44444444444444\n",
      "Saving best model...\n",
      "Done.\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 7 || Run Time:    4.7 | Load Time:    0.6 || F1:  96.20 | Prec:  93.83 | Rec:  98.70 || Ex/s:  60.35\n",
      "\n",
      "===>  EVAL Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [█] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 7 || Run Time:    0.6 | Load Time:    0.2 || F1:  88.46 | Prec:  82.14 | Rec:  95.83 || Ex/s: 136.17\n",
      "\n",
      "* Best F1: 88.46153846153845\n",
      "Saving best model...\n",
      "Done.\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 8 || Run Time:    4.8 | Load Time:    0.6 || F1: 100.00 | Prec: 100.00 | Rec: 100.00 || Ex/s:  60.15\n",
      "\n",
      "===>  EVAL Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [█] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 8 || Run Time:    0.6 | Load Time:    0.2 || F1:  87.50 | Prec:  87.50 | Rec:  87.50 || Ex/s: 133.42\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 9 || Run Time:    4.9 | Load Time:    0.6 || F1: 100.00 | Prec: 100.00 | Rec: 100.00 || Ex/s:  58.39\n",
      "\n",
      "===>  EVAL Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [█] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 9 || Run Time:    0.8 | Load Time:    0.2 || F1:  85.71 | Prec:  84.00 | Rec:  87.50 || Ex/s: 106.88\n",
      "\n",
      "---------------------\n",
      "\n",
      "===>  TRAIN Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [████] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 10 || Run Time:    5.1 | Load Time:    0.6 || F1: 100.00 | Prec: 100.00 | Rec: 100.00 || Ex/s:  56.64\n",
      "\n",
      "===>  EVAL Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [█] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Epoch 10 || Run Time:    0.6 | Load Time:    0.2 || F1:  85.71 | Prec:  84.00 | Rec:  87.50 || Ex/s: 129.94\n",
      "\n",
      "---------------------\n",
      "\n",
      "Loading best model...\n",
      "Training done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "88.46153846153845"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_model.run_train(\n",
    "    train,\n",
    "    validation,\n",
    "    epochs=10,\n",
    "    batch_size=16,\n",
    "    best_save_path='hybrid_model.pth',\n",
    "    pos_neg_ratio=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.5 extract intermediate layers\n",
    "In this step we want to evaluate the output of intermediate layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to save intermediate layer output/input\n",
    "class Hook():\n",
    "    def __init__(self, module, backward=False):\n",
    "        if backward==False:\n",
    "            self.hook = module.register_forward_hook(self.hook_fn)\n",
    "        else:\n",
    "            self.hook = module.register_backward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.input = input\n",
    "        self.output = output\n",
    "    def close(self):\n",
    "        self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:deepmatcher.data.dataset:Rebuilding data cache because: ['One or more data files have been modified.']\n",
      "\n",
      "Reading and processing data from \"sample_data/itunes-amazon/train.csv\"\n",
      "0% [############################# ] 100% | ETA: 00:00:00\n",
      "Reading and processing data from \"sample_data/itunes-amazon/altered_positives_samples.csv\"\n",
      "0% [############################# ] 100% | ETA: 00:00:00\n",
      "Reading and processing data from \"sample_data/itunes-amazon/positives_samples.csv\"\n",
      "0% [############################# ] 100% | ETA: 00:00:00\n",
      "Building vocabulary\n",
      "0% [#] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n",
      "\n",
      "Computing principal components\n",
      "0% [#] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:00\n"
     ]
    }
   ],
   "source": [
    "positives = dm.data.process(path='sample_data/itunes-amazon/',train='train.csv',\n",
    "                            validation='altered_positives_samples.csv',test='positives_samples.csv',cache='pcache.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepmatcher.data import MatchingIterator\n",
    "batch_size = 10\n",
    "positives_splits = MatchingIterator.splits(positives,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives_batches = [] \n",
    "for batch in positives_splits[2]:\n",
    "    positives_batches.append(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "altered_positives = []\n",
    "for batch in positives_splits[1]:\n",
    "    altered_positives.append(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-3.5324 -0.0297\n",
       "-1.5120 -0.2491\n",
       "-2.0955 -0.1313\n",
       "-3.8832 -0.0208\n",
       "-3.5521 -0.0291\n",
       "-0.1272 -2.1246\n",
       "-0.9190 -0.5090\n",
       "-3.9984 -0.0185\n",
       "-2.3539 -0.0998\n",
       "-1.2216 -0.3492\n",
       "[torch.cuda.FloatTensor of size 10x2 (GPU 0)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_model.forward(positives_batches[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-3.3979 -0.0340\n",
       "-1.7124 -0.1990\n",
       "-0.2488 -1.5130\n",
       "-1.6829 -0.2056\n",
       "-0.1177 -2.1977\n",
       "-0.3870 -1.1367\n",
       "-0.3977 -1.1143\n",
       "-4.2716 -0.0141\n",
       "-0.2067 -1.6781\n",
       "-0.6373 -0.7523\n",
       "[torch.cuda.FloatTensor of size 10x2 (GPU 0)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_model.forward(altered_positives[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = ['Song_Name','Artist_Name','Album_Name','Price','CopyRight','Released','Genre','Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizers = []\n",
    "comparators = []\n",
    "for attr in attributes:\n",
    "    summarizers.append(hybrid_model.attr_summarizers[attr])\n",
    "    comparators.append(hybrid_model.attr_comparators[attr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "hookF = []\n",
    "for summ in summarizers:\n",
    "    hookF.append(Hook(summ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_layer_input_output(hook_functions,batch,model):\n",
    "    out= model(batch)\n",
    "    layer_inputs = []\n",
    "    layer_outputs = []\n",
    "    for hook in hook_functions:\n",
    "        layer_inputs.append(hook.input)\n",
    "        layer_outputs.append(hook.output)\n",
    "    return layer_inputs,layer_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives_batch_layer_inputs,positives_batch_layer_outputs = return_layer_input_output(hookF,\n",
    "                                                                                     positives_batches[1],hybrid_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "altered_batch_layer_inputs,altered_batch_layer_outputs = return_layer_input_output(hookF,\n",
    "                                                                                  altered_positives[1],hybrid_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives_summarizers_left_output = list(map(lambda x: x[0].data,positives_batch_layer_outputs))\n",
    "positives_summarizers_right_output = list(map(lambda x: x[1].data,positives_batch_layer_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "altered_summarizers_left_output = list(map(lambda x: x[0].data,altered_batch_layer_outputs))\n",
    "altered_summarizers_right_output = list(map(lambda x:x[1].data,altered_batch_layer_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_matrix(summarizers_left_output,summarizers_right_output):\n",
    "    distance_mat = []\n",
    "    for i in range(len(summarizers_left_output)):\n",
    "        distances = []\n",
    "        for j in range(batch_size):\n",
    "            l_out = summarizers_left_output[i][j].data\n",
    "            r_out = summarizers_right_output[i][j].data\n",
    "            dist = distance.euclidean(l_out,r_out)\n",
    "            distances.append(dist)\n",
    "        distance_mat.append(distances)\n",
    "    distance_mat = np.matrix(distance_mat)\n",
    "    return distance_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_mat_positives = calculate_distance_matrix(positives_summarizers_left_output,positives_summarizers_right_output)\n",
    "distance_mat_positives = distance_mat_positives.reshape((10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_positives_df = pd.DataFrame(data = distance_mat_positives,columns=attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Song_Name</th>\n",
       "      <th>Artist_Name</th>\n",
       "      <th>Album_Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>CopyRight</th>\n",
       "      <th>Released</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.818525</td>\n",
       "      <td>0.353629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.539159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.180276</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.224907</td>\n",
       "      <td>1.275132</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.741696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.650884</td>\n",
       "      <td>0.099023</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.391206</td>\n",
       "      <td>0.005628</td>\n",
       "      <td>1.020680</td>\n",
       "      <td>0.720574</td>\n",
       "      <td>0.558888</td>\n",
       "      <td>0.005542</td>\n",
       "      <td>0.814779</td>\n",
       "      <td>0.808317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.699374</td>\n",
       "      <td>0.628647</td>\n",
       "      <td>0.991943</td>\n",
       "      <td>0.512151</td>\n",
       "      <td>0.625245</td>\n",
       "      <td>0.524573</td>\n",
       "      <td>0.596789</td>\n",
       "      <td>0.657549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.023974</td>\n",
       "      <td>0.655507</td>\n",
       "      <td>0.910311</td>\n",
       "      <td>0.588852</td>\n",
       "      <td>1.718542</td>\n",
       "      <td>1.337025</td>\n",
       "      <td>0.328442</td>\n",
       "      <td>1.655995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.154017</td>\n",
       "      <td>0.246117</td>\n",
       "      <td>0.562215</td>\n",
       "      <td>2.061483</td>\n",
       "      <td>2.381361</td>\n",
       "      <td>0.872756</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.202635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.343569</td>\n",
       "      <td>2.266645</td>\n",
       "      <td>3.207994</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Song_Name  Artist_Name  Album_Name     Price  CopyRight  Released  \\\n",
       "0   0.000000     0.818525    0.353629  0.000000   0.000000  0.539159   \n",
       "1   0.000000     0.180276    0.000000  0.000000   0.000000  0.000000   \n",
       "2   0.000000     0.000000    0.000000  0.000000   0.224907  1.275132   \n",
       "3   0.650884     0.099023    0.000040  0.000090   0.000023  0.000039   \n",
       "4   0.000000     0.000000    0.000000  0.000000   0.000000  0.000000   \n",
       "5   0.391206     0.005628    1.020680  0.720574   0.558888  0.005542   \n",
       "6   0.699374     0.628647    0.991943  0.512151   0.625245  0.524573   \n",
       "7   1.023974     0.655507    0.910311  0.588852   1.718542  1.337025   \n",
       "8   2.154017     0.246117    0.562215  2.061483   2.381361  0.872756   \n",
       "9   2.202635     0.000000    0.000000  2.343569   2.266645  3.207994   \n",
       "\n",
       "      Genre      Time  \n",
       "0  0.000000  0.000000  \n",
       "1  0.000000  0.000000  \n",
       "2  0.000026  0.741696  \n",
       "3  0.000000  0.000000  \n",
       "4  0.000000  0.000000  \n",
       "5  0.814779  0.808317  \n",
       "6  0.596789  0.657549  \n",
       "7  0.328442  1.655995  \n",
       "8  0.000000  0.000000  \n",
       "9  0.000000  0.000000  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances_positives_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_mat_altered = calculate_distance_matrix(altered_summarizers_left_output,altered_summarizers_right_output)\n",
    "distance_mat_altered = distance_mat_altered.reshape((10,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_altered_df = pd.DataFrame(data=distance_mat_altered,columns = attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Song_Name</th>\n",
       "      <th>Artist_Name</th>\n",
       "      <th>Album_Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>CopyRight</th>\n",
       "      <th>Released</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.818525</td>\n",
       "      <td>0.353629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.539159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.180276</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.224907</td>\n",
       "      <td>1.275132</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.741696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.650884</td>\n",
       "      <td>0.099023</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.391206</td>\n",
       "      <td>0.005628</td>\n",
       "      <td>1.020680</td>\n",
       "      <td>0.720574</td>\n",
       "      <td>0.558888</td>\n",
       "      <td>0.005542</td>\n",
       "      <td>0.814779</td>\n",
       "      <td>0.808317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.699374</td>\n",
       "      <td>0.628647</td>\n",
       "      <td>0.991943</td>\n",
       "      <td>0.512151</td>\n",
       "      <td>0.625245</td>\n",
       "      <td>0.524573</td>\n",
       "      <td>0.596789</td>\n",
       "      <td>0.657549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.023974</td>\n",
       "      <td>0.655507</td>\n",
       "      <td>0.910311</td>\n",
       "      <td>0.588852</td>\n",
       "      <td>1.718542</td>\n",
       "      <td>1.337025</td>\n",
       "      <td>0.328442</td>\n",
       "      <td>1.655995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.154017</td>\n",
       "      <td>0.246117</td>\n",
       "      <td>0.562215</td>\n",
       "      <td>2.061483</td>\n",
       "      <td>2.381361</td>\n",
       "      <td>0.872756</td>\n",
       "      <td>2.629158</td>\n",
       "      <td>0.549303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.141140</td>\n",
       "      <td>0.455519</td>\n",
       "      <td>2.756165</td>\n",
       "      <td>0.690089</td>\n",
       "      <td>0.510815</td>\n",
       "      <td>3.582113</td>\n",
       "      <td>3.026195</td>\n",
       "      <td>1.469071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Song_Name  Artist_Name  Album_Name     Price  CopyRight  Released  \\\n",
       "0   0.000000     0.818525    0.353629  0.000000   0.000000  0.539159   \n",
       "1   0.000000     0.180276    0.000000  0.000000   0.000000  0.000000   \n",
       "2   0.000000     0.000000    0.000000  0.000000   0.224907  1.275132   \n",
       "3   0.650884     0.099023    0.000040  0.000090   0.000023  0.000039   \n",
       "4   0.000000     0.000000    0.000000  0.000000   0.000000  0.000000   \n",
       "5   0.391206     0.005628    1.020680  0.720574   0.558888  0.005542   \n",
       "6   0.699374     0.628647    0.991943  0.512151   0.625245  0.524573   \n",
       "7   1.023974     0.655507    0.910311  0.588852   1.718542  1.337025   \n",
       "8   2.154017     0.246117    0.562215  2.061483   2.381361  0.872756   \n",
       "9   1.141140     0.455519    2.756165  0.690089   0.510815  3.582113   \n",
       "\n",
       "      Genre      Time  \n",
       "0  0.000000  0.000000  \n",
       "1  0.000000  0.000000  \n",
       "2  0.000026  0.741696  \n",
       "3  0.000000  0.000000  \n",
       "4  0.000000  0.000000  \n",
       "5  0.814779  0.808317  \n",
       "6  0.596789  0.657549  \n",
       "7  0.328442  1.655995  \n",
       "8  2.629158  0.549303  \n",
       "9  3.026195  1.469071  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances_altered_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
